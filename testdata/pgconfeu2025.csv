id,title,speaker,abstract,track,score
PGCONF_7176,The Foundation of Open Source: Why Software Freedom?,Karen Sandler,"Open source has become the backbone of society's software. Critical projects like PostgreSQL power some of the most important software in the industry. While many aspects of Open Source make software successful, such as its collaborative development model and price, these features are all underpinned and guaranteed by the software freedom that Open Source licenses provide. In this talk, Karen will explore why software freedom is essential to modern day computing, as well as to every day life.",PGConf.EU,
PGCONF_6838,Don't do that!,Laurenz Albe,"Rather than presenting best practices, I'll show a number of design choices that have repeatedly caused problems for users. Learn from the errors of others! Topics include: - choosing the wrong data type - choosing ""integer"" as data type for primary keys - defining columns nullable - using Large Objects - defining dangerous CHECK constraints - lying about IMMUTABLE - choosing an entity-attribute-value design",App Developer (45 minutes),
PGCONF_7138,From Stars to Storage Engines: Migrating Big Science Workloads Beyond Greenplum,Joaquim Oliveira,"We archive massive datasets to map the stars in ESA’s Gaia mission and unlock the secrets of dark matter and dark energy with Euclid. Today, these are living on Greenplum — but shifting away from Open Source Software demands have us exploring new directions: migrating to Citus or adopting EDB’s next-generation Greenplum-based warehouse. In this talk, we’ll share how we’re rethinking architecture, tools, and operations to handle petabyte-scale astronomy workloads. We’ll cover the practical pros and cons of each option, the administrative shifts required, and the hurdles we face moving such critical operations between engines. Attendees will leave with real-world lessons and key questions to ask before starting a migration of their own.",DBA (45 minutes),
PGCONF_6935,"Behind Postgres 18: The People, the Code, & the Invisible Work",Claire Giordano,"Ever wonder who makes Postgres happen? Not just the new features in Postgres 18—but the people, the processes, and the invisible work that brings each new release to life? Let’s find out. In an open source project like Postgres, recognizing the behind-the-scenes volunteer work isn’t just nice—it’s necessary. This revamped version of last year’s PGConf EU talk shares a fresh analysis of the contributions to Postgres in the v18 timeframe. Like last year’s talk, this research is done by Postgres committer Daniel Gustafsson and Claire Giordano—and builds on community data sources including commit logs, mailing lists, and the PostgreSQL.org site. You’ll see maps and charts as you learn about the people behind governance, conferences, meetups, infrastructure, and the code. But this year there will be new insights—and more stories. Like: who translates all the error messages? And what went into making PGConf.dev feel so magical? You’ll leave with a deeper appreciation for the full spectrum of work behind a Postgres release—and a few concrete ideas for how we can better recognize the people who make it happen. Because when we recognize the people doing this work, it’s more than a thank you—it’s how we grow the next generation of contributors.",Community (45 minutes),
PGCONF_7177,From Transactions to Intelligence: Postgres for Open AI & Analytics,Torsten Steinbach,"Postgres has thrived for decades as the world’s most trusted open-source database, powering an extraordinary share of transactional workloads across industries. But the data landscape is shifting: analytics and AI demand far richer data modalities, more diverse programming models, and new ways of consuming data. To meet this future, Postgres must not only endure — it must evolve. In this talk, we’ll explore three powerful pathways driving that evolution. First, the organic path, where Postgres itself grows new capabilities — think WarehousePG for analytics or pgvector and friends for AI. Second, the hybrid path, where Postgres joins forces with other open-source engines like DataFusion and open metadata frameworks such as Iceberg REST catalogs. And third, the augmented path, where Postgres extends into entirely new ecosystems — becoming a first-class citizen in agentic AI through robust MCP-driven tooling.",Sponsors,
PGCONF_7067,Anonymizing Data in Postgres Without Losing Your Mind (or Your Schema),"Ahmet Gedemenli, Esther Miñano Sanz","Scrubbing sensitive data in PostgreSQL can feel like a chore, especially when you're trying to do it safely, flexibly, and without rewriting your entire schema. Many teams face the need for column-level anonymization when working with production clones, analytics pipelines, or complying with regulations like GDPR. Doing it well can be a real challenge. In this talk, we’ll walk through modern, practical ways to anonymize data in Postgres. We’ll cover how to mask or hash specific columns, handle nested fields (like JSON), and even apply different rules based on data values, all without touching the original database structure. You’ll see examples of redaction, pseudonymization, and other transformations that keep your data useful while protecting what matters. We’ll also compare popular open-source tools like pg_anonymize, known for in-database anonymization, with platforms like Greenmask and NeoSync that focus on masking and secure data syncing. Some of these tools are integrated directly into pgstream, enabling seamless, real-time data transformations as part of logical replication workflows. To bring it all together, we’ll look at how pgstream, an open-source tool that uses logical replication and supports plug-and-play data transformers, can help apply these anonymization techniques in real time. pgstream integrates with several of the open-source tools mentioned earlier, allowing you to set up custom or built-in anonymization rules with simple configuration and without requiring major changes to your database. If you’ve ever needed to clean up sensitive data before sending it downstream (or just wanted a cleaner way to do it), this talk will give you the tools and ideas to make it happen.",App Developer (25 minutes),
PGCONF_6975,Parsing Postgres logs the non-pgBadger way,Kaarel Moppel,"pgBadger has been a staple in the community for over a decade, and deserves a nod for sure. But at the same time one also has to acknowledge that the world has changed a ""bit"" during that time. Are there some log parsing use cases that are better served with something else? What are the current options besides ad-hoc grep? This talk is a look at a fresh effort to provide an alternative Open Source log parser to complement pgBadger. We'll look at how pgweasel tries to be: - way faster - way simpler - concentrating on the Pareto DBA flow and the CLI experience - more cloud-friendly - require zero configuration - more user-friendly",DBA (25 minutes),
PGCONF_7009,Data modeling with PostgreSQL at the core of Software Development,Boriss Mejias,"Data structures and their relationships are key to software design. For most, how data is organized is even more important than the application code itself, because the code, the flowcharts and the processes will be derived and become straightforward from the data structure and their relationships. Learning how to model your data and how to design your schema in PostgreSQL becomes fundamental to create a successful software project. How would you store and organize the data? Which data types would you use? Will there be any constraints? And most importantly, which types of questions will your application be able to answer directly from the data? Honoring Mikhail Tal, the Magician from Riga, eighth World Chess Champion, in this talk we will go through the design of an application to track the results of a chess tournament. You will learn some principles of data modeling, letting PostgreSQL guarantee data integrity. We will see how to build the business logic of the application in the data model itself, giving you extra powers as a software developer.",App Developer (25 minutes),
PGCONF_7008,Boosting Typical Query Patterns: PostgreSQL 18's Performance Enhancements,Roberto Mello,"As with previous releases, PostgreSQL 18 introduces many performance improvements that directly benefit everyday database tasks. Beyond doing an overview of the performance improvements, this talk will go over these changes and compare the performance of popular applications using PostgreSQL 16, 17 and 18, to highlight the improvements in PG 18. The optimizer now automatically eliminates unnecessary self-joins, converts IN (VALUES ...) to x = ANY ... for better statistics, and transforms OR-clauses into arrays for faster index processing. It also speeds up INTERSECT, EXCEPT, window aggregates, and optimizes SELECT DISTINCT and GROUP BY queries by reordering keys and ignoring redundant columns. This talk will walk through these and many more, while showing data from running applications.",App Developer (45 minutes),
PGCONF_6982,"Vector data in Postgres: Size, TOAST, Filters and Performance",Gleb Otochkin,"AI applications are changing how we use databases, and vector data is at the center of this change. By enabling semantic search within domain datasets, vectors allow for more intelligent and nuanced queries. But managing this data effectively requires understanding the challenges of vector size and its impact on storage and performance. This presentation explores practical strategies for storing, indexing, and querying vector data in operational databases and different factors impacting performance and management. Learn how to optimize your database for AI-powered applications and unlock the full potential of vector search.",DBA (45 minutes),
PGCONF_6952,Improved Freezing in Postgres Vacuum: From Idea to Commit,Melanie Plageman,"This talk uses the recently committed eager scanning feature as a case study to walk through the Postgres feature development process. It’s meant to give prospective contributors a realistic look at what it takes to get a feature committed and to help users better understand how Postgres evolves and how they can benefit from these changes. Anti-wraparound vacuums are one of the most consistent Postgres user complaints. This talk explains how freeze debt accumulates, how it varies based on table access patterns, and how Postgres attempts to manage it. Eager scanning started as a proposal on pgsql-hackers to preemptively freeze modified pages using per-table access pattern statistics. Over years of discussion and experimentation (including evaluating and discarding many empirical models and approaches), the shape of both the problem and solutions evolved. This talk dives into what made designing an adaptive feature in Postgres so difficult: extreme constraints, second-order effects, code complexity, and the sheer variety of workloads Postgres must support. The final committed feature is simpler than the original idea: it eagerly scans all-visible but not all-frozen pages to amortize the cost of aggressive vacuums. It doesn’t try to predict access patterns or freeze lifespan, but it’s effective, maintainable, and is a step toward addressing the core problem. Finally, the talk will explain how users can expect this change to reduce the frequency and impact of anti-wraparound vacuums in their own workloads.",Postgres Internals (45 minutes),
PGCONF_7180,What we built with Open Source,Hans-Jürgen Schönig,"After more than 25 years working with PostgreSQL, we’ve seen it all, from clever hacks to some of the most ambitious deployments out there. PostgreSQL’s rich ecosystem makes it possible to cover just about every need, including monitoring, tuning, automation, compliance, scalability, and high availability. In this talk, we’ll take you behind the scenes and show you what we’ve built along the way: open-source contributions, customer solutions, and custom projects designed to solve real-world problems. Join us for a tour of cool ideas, battle-tested tools, and lessons learned from the field.",Sponsors,
PGCONF_6800,Practical Guide to Migrating +100 Large PostgreSQL to the Cloud,Nelson Calero,"Migrating a large fleet of PostgreSQL databases to a managed service is a journey filled with opportunities and potential pitfalls. This is especially true when dealing with numerous multi-terabyte databases and particular cases such as migrating from an old version, using inheritance, PostGIS, moving to GCP Service accounts, and changing database assignment to instances. This session shares the real-world experience of migrating more than 100 PostgreSQL databases, ranging in size from gigabytes to multiple terabytes, from traditional virtual machines to a managed cloud service, specifically Google Cloud SQL for PostgreSQL. We will share lessons learned, covering planning, execution, and problems encountered, with practical advice to save you time, resources, and headaches during a similar migration. Key discussion areas will include tooling selection, pre-migration assessments, choosing the right instance type, optimizing the migration process for large datasets, strategies for minimizing downtime, and essential post-migration validation and performance tuning. This session is a must-attend for anyone considering or currently executing a large-scale PostgreSQL migration to a managed service and wants to learn from real-world successes and challenges.",DBA (45 minutes),
PGCONF_6817,Operational hazards of managing PostgreSQL DBs over 100TB,Teresa Lopes,"Picture this: you start a new role, eager to learn and contribute with your ideas! Your next task is to get familiar with the database setup, and then you start encountering these massive PostgreSQL databases — 100TB, 200TB, 300TB... And you start questioning yourself: how do you backup (and restore) a +100TB database? And how about HA? Performance? Vacuum? It should work the same way as for a 100GB database, right? Well, maybe not exactly. Blog posts and best practice guides make PostgreSQL seem straightforward—until you push it to its limits. At extreme scale, you will find yourself questioning the most fundamental assumptions about how PostgreSQL works. Over the last years, my team at Adyen has been exploring the boundaries of what PostgreSQL can do, and today I will share our findings with you (at least the ones I can!).",DBA (45 minutes),
PGCONF_7103,"Barman, past, present, and future of a pioneer community project","Giulio Calacoci, Martín Marqués","Barman is one of the first Backup and Recovery management tool for Postgres, released as an open-source tool by 2ndQuadrant, with lots of adopters in the last decade. In this talk, we will go over the history of Barman as a community project, from its beginning to the current state of affairs. We will be showing the great achievements, the pain-point moments, and the future. We hope to: * Enlighten people with Backup and Recovery insights (yes, we will talk about Backups) * Share how the teams have interacted with the community, balancing the customer requests with community proposals * Go over these 13 years of open source development * Describe where we plan to take the project in the future, in terms of development features, and engagement with the community We hope to leave an itch to either join our community or start your own community project, preferably in the Postgres ecosystem.",Community (45 minutes),
PGCONF_7184,Unlock PostgreSQL’s True Performance with Local SSDs,Burak Yucesoy,"Cloud storage was built around the limits of old hardware. Spinning hard drives (HDDs) were slow and fragile. So, early on, cloud providers moved storage off of servers. They used network-attached disks to boost durability and scalability. But hardware has evolved. Today, you can get 2.5 million IOPS from a $600 NVMe SSD. Achieving the same throughput through network-attached storage in the cloud can cost hundreds of thousands of dollars per month. With NVMe SSDs now faster, cheaper, and more reliable, it's time to rethink PostgreSQL storage. In this talk, we’ll walk through different disk architectures and their impacts on PostgreSQL performance, how we got here, what’s changed, and why local NVMe SSDs are the future for PostgreSQL storage. We'll also share benchmarks comparing performance on different disk architectures and share results from real life case studies.",Sponsors,
PGCONF_6868,Normalize or De-normalize? Relational SQL Columns or JSON Document Attributes?,Franck Pachot,"Data modeling has progressed beyond traditional SQL Normal Forms and the unstructured storage of early NoSQL systems. Modern applications demand flexible data models that merge normalized entities with semi-structured documents. Contemporary SQL databases now support binary JSON columns and offer APIs similar to MongoDB (such as FerretDB for PostgreSQL). Furthermore, NoSQL databases have evolved to allow normalized collections and provide transactional operations. However, key questions remain: When should multiple entities be embedded within a single JSONB document? And when is it more efficient to define a relational column instead of a sub-document attribute? This session will clarify the distinction between physical and logical data modeling. Denormalization can enhance physical storage by placing frequently accessed data together, while normalization is essential for maintaining logical data integrity and preventing issues related to redundancy. By the end of this session, you will understand how to balance these two approaches to create data models that are efficient, maintainable, and adaptable to changing application requirements.",App Developer (45 minutes),
PGCONF_7133,What You Should Know About Constraints in PostgreSQL (and What’s New in 18),Gülçin Yıldırım Jelínek,"PostgreSQL 18 introduces significant enhancements to constraints, your first line of defense for maintaining data integrity. This talk focuses on the new capabilities brought by version 18, including non-overlapping PRIMARY KEY and UNIQUE constraints, named NOT NULL constraints, NOT ENFORCED constraints, and improved support for partitioned tables. We’ll look at what’s new, why it matters, and how to apply these features in real-world systems. We will begin with a brief refresher on the different types of constraints (CHECK, NOT NULL, UNIQUE, PRIMARY KEY, FOREIGN KEY) to help you get the most out of PostgreSQL’s declarative integrity model. Then we’ll go into details of what's new in PostgreSQL 18, including: * Named NOT NULL constraints and their inclusion to foreign tables * NOT ENFORCED constraints for CHECK and foreign keys * WITHOUT OVERLAPS * ALTER TABLE ... ALTER CONSTRAINT ... [NO] INHERIT * Enhanced handling of constraints on partitioned tables * Requirements around deterministic/nondeterministic collations in primary/foreign key relationships",DBA (45 minutes),
PGCONF_7056,What implementing pg_tde taught us about PostgreSQL,Jan Wieremjewicz,"This is a firsthand account of bringing Transparent Data Encryption to PostgreSQL through pg_tde. From idea to patch proposals, it’s a story of navigating PostgreSQL’s internals, Community realities, and trade-offs between extension and core changes. Why weren’t existing hooks enough? What friction did we hit? How was the experience with the Community feedback cycle? What customer feedback shaped the final design and how did users react to the proposed solutions? Based on years of work to deliver a critical enterprise capability, this talk is a diary of what it took to deliver Transparent Data Encryption as an extension to PostgreSQL, from a product manager who lived through it.",Community (45 minutes),
PGCONF_7179,DataGrip: A Powerful IDE for PostgreSQL,Maxim Sobolevskiy,"In this talk, Maxim Sobolevskiy is going to show what DataGrip is all about, from the basics to more complicated things like AI for SQL, databases in version control, and scripted mechanisms for data export. What will be covered: – Various types of IntelliSense, code snippets, expanding wildcards, and subqueries. – AI Assistant: Text-to-SQL, Fix with AI, and AI-based export. – Data editor: Editable result-sets, bulk and multiple submit, and transposing data. – Data import/export: Import/export from CSV, JSON, Excel, and INSERT statements, and export to other databases. – Navigation: Searching for database objects, data, SQL code, files, settings, and more. – Additional tools: Diagrams, user-defined parameters, and a powerful diff tool. DataGrip is built with one purpose in mind: to save developers from repetitive work. Join this session to find out how well it fulfills that purpose.",Sponsors,
PGCONF_7046,Hacking pgvector for performance,Daniel Krefl,"In this talk I will present a modification to the popular pgvector Postgres extension for approximate nearest neighbor search. The modified extension keeps the vector index persistent in external (Non-Postgres) memory, and pushes potential filter conditions directly into the index scan. Storage of index and compute can also be offloaded to a GPU. I will explain in detail the inner workings of a Postgres index scan, the rational behind moving core parts of pgvector away from Postgres, and the performance implications. I will also highlight how this modified extension finds application in massive data processing within the framework of the AERO project for the future heterogeneous EU cloud infrastructure.",App Developer (45 minutes),
PGCONF_6961,Patroni and pgBackRest: better together,Stefan Fercot,"Even in a world of managed database services and cloud-native platforms, Patroni and pgBackRest remain two essential components in the PostgreSQL operations toolbox. This talk is your go-to guide for combining the strengths of this powerful duo to achieve high availability and disaster recovery. You will learn how to integrate pgBackRest into a Patroni-managed cluster, safely rebuild standby nodes, bootstrap an entire cluster from an existing pgBackRest backup, and perform point-in-time recovery (PITR) entirely under Patroni's control. Expect real-world examples, detailed YAML configuration snippets, and the kind of operational wisdom that only comes from lessons learned the hard way.",DBA (45 minutes),
PGCONF_6989,All in a context!,Rafia Sabih,"Postgresql has a special way of handling memory management, which is via specialized data structure called memory context. All the memory allocation and deallocation are done using a memory context. One never requires independent calls to memory functions like free, malloc, etc. in PostgreSQL. Memory contexts are hierarchical in nature, so deleting a parent context deletes and deallocates all the children contexts, this eases the memory handling by avoiding memory leaks. There are also some specialised memory contexts defined like MessageContext, TopTransactionContext, PortalContext, ErrorContext with their specific purposes. There are APIs available to handle the working of memory contexts to create, delete, reallocate, deallocate, reset, or switch to other contexts. There are also APIs available to identify the current context or switch to some globally available memory contexts. This talk aims at discussing the details of memory management in PostgreSQL.",Postgres Internals (25 minutes),
PGCONF_7181,Exploring new enhancements and improvements Fujitsu has delivered to PostgreSQL,Vincent O'Dea,"In this session we will dive into a series of recent contributions to PostgreSQL, made possible by the Fujitsu team working with sustained focus on core database features. We will explore some of these new enhancements and improvements including: 1. Advanced Conflict Management 2. Parallel Apply for High Throughput 3. Table Exclusions 4. Seamless Cluster Upgrades 5. Slot Synchronization for High Availability",Sponsors,
PGCONF_7229,PostgreSQL Europe Reception,,"As with previous PGConf.EU events, the 2025 conference in Riga will have a social event. Following the success of last year's event, we’re again making this year’s social easy for all to attend: just join us in the common area at the conclusion of the conference on Wednesday, October 22, for snacks, refreshments and activities. We look forward to seeing you and celebrating what we know will be a great conference!",PGConf.EU,
PGCONF_6960,"Have you ever wondered, how a cloud platform works?","Maximilian Stefanac, Philipp Thun","Cloud Foundry is an open-source Platform as a Service project which is, that serves as the workhorse of the SAP Business Technology Platform, powered by PostgreSQL. Database Performance is crucial for Cloud Foundry because it has direct effects to the performance and scalability of the installed platform which is crucial for its operations and high-demanding enterprise users. Consequently, SAP contributes actively to optimizations of queries for PostgreSQL in Cloud Foundry to operate the Cloud Platform at a large scale. Over the years the Cloud Foundry team at SAP, gained experience in how to set up and run reliable and performant cloud foundry installations on AWS, Azure, Google Cloud and Alibaba Cloud. From outside the offerings looks similar, but you do not get similar offerings: This is especially a challenge in unified automation and monitoring of PostgreSQL databases on these infrastructures. Attendees will learn about - Cloud Foundry in general and the usage of PostgreSQL in this open-source project. - Running PostgreSQL on different cloud infrastructures -> comparison of the different service offerings. - PostgreSQL optimizations for large scale cloud foundry installations in the code and in the database server. It is the foundation for some open-source contributions of SAP in this area. - Zero-downtime Schema Migrations in high load scenarios - Some true live stories about PostgreSQL major version upgrades which inexpertly caused problems.",App Developer (45 minutes),
PGCONF_7035,Unified Observability: Monitoring Postgres Anywhere with OpenTelemetry,Yogesh Jain,"PostgreSQL is widely used across cloud, Kubernetes, and bare metal environments. But how can we monitor it efficiently across all these setups without vendor lock-in or high costs? In this talk, we'll explore how OpenTelemetry helps achieve consistent and scalable observability for Postgres - no matter where it runs. We'll walk through how to collect metrics and logs, integrate them with open-source observability tools, and optimize monitoring for both performance and cost. Expect live demos and practical insights on making PostgreSQL monitoring simple, efficient, and future-ready with OpenTelemetry. Key Takeaways: - How OpenTelemetry brings consistent Postgres observability across cloud, kubernetes, and bare metal - How to collect telemetry data (metrics and logs) for effective Postgres monitoring - How to use open-source tools (Prometheus, Loki, Grafana) to observe Postgres - Tips to reduce observability costs while keeping full visibility - Live demo of Postgres monitoring in cloud-native environments",DBA (45 minutes),
PGCONF_6908,Why PostgreSQL people should really care about Kafka and Debezium?,Dirk Krautschick,"In the realm of data management, the convergence of Postgres, Kafka, and Debezium heralds a new era of possibilities. This talk is a compelling exploration into why individuals immersed in the Postgres ecosystem must pay careful attention to Kafka and Debezium, unveiling the transformative impact of this trinity on data architecture. The discussion kicks off by unraveling the unique strengths of each player - obviously Postgres as a robust relational database, Kafka as a resilient event streaming platform and Debezium as the linchpin enabling change data capture (CDC). Attendees will embark on a journey through the intricacies of integrating these technologies, understanding how their synergy facilitates real-time data propagation and synchronization. Throughout the talk, we'll dive into the benefits that arise when Postgres enthusiasts embrace Kafka and Debezium, from unlocking unparalleled insights into database changes to fortifying data consistency across distributed systems. Practical insights and real-world examples will illuminate the potential pitfalls in this integration, coupled with strategies to navigate challenges effectively. This talk aspires to empower Postgres aficionados to navigate the evolving data terrain with confidence, fostering a community that recognizes the symbiotic relationship between Postgres, Kafka, and Debezium as the key to unlocking the full potential of their data ecosystems.",DBA (45 minutes),
PGCONF_7187,DBtune: AI-driven performance tuning for all PostgreSQL flavors,Dr. Luigi Nardi,"Manual PostgreSQL tuning is an error prone guessing game resulting in volatile performance, costly over-provisioning, and unnecessary downtime. For enterprises managing large, heterogeneous database fleets, this problem is magnified, causing spiraling infrastructure costs and unpredictable behavior. This session introduces the new major release of the DBtune and address the challenge of tuning complex workloads for enterprises. Learn how DBtune's autonomous agent analyzes your unique workloads and adapts server parameters to maximize your database’s resource utilization and throughput. We will directly compare DBtune's adaptive approach to static methods like PGTune. Through a live demo, we will show you how DBtune operates seamlessly across any environment, from self-hosted community PostgreSQL to managed cloud services like Amazon RDS, Aurora, and Google Cloud SQL. You will discover practical strategies to automate tuning, achieve predictable high performance, and cost optimization.",Sponsors,
PGCONF_6795,EXPLAIN: make it make sense,Aivars Kalvāns,"Most developers know how to measure the time a database query takes and to find which one is the slowest. But what next? What is going on there? Is it good that it’s doing that? Is the query supposed to take this much time? Will some indexes help? In this talk, I will help you decipher database query plans and give some rules of thumb to understand if the database is doing the best it can. We will also use query plans to find the best indexes for queries by observing what the database is looking for. I will also share several anti-patterns I have seen in projects and show how to rewrite them in a database-friendly way. Although I will use Python to explain the principles and execute my queries, I will show the raw SQL to make it applicable to any programming language.",App Developer (25 minutes),
PGCONF_7134,Kubernetes from the Database Out,Alastair Turner,"You already know your Postgres, but you're being asked to work with this Kubernetes thing. Do you want to focus your Kubernetes learning on only the bits which are relevant to databases? Then this is the talk for you! Skip the usual 'why run databases on Kubernetes' discussion and dive straight into what you need to know to talk Postgres on Kubernetes. Using a Postgres database hosted on Kubernetes as an example, I will cover the components of Kubernetes involved in day-to-day activities - including network connectivity, storage, and the automation of restarts and upgrades. Join me for an overview of the few, core terms and concepts among the many, many pieces of the Kubernetes ecosystem you need to learn about first, to start your Data on Kubernetes journey.",DBA (25 minutes),
PGCONF_6947,Discover your contribution-magic,Cornelia Biacsics,"Let's discover some ways you can make individual contributions to the PostgreSQL project besides the traditional development paths, because everyone who has a point of contact with PostgreSQL is invited to do so. Learn more about how you can contribute with your personal skills and knowledge, even without a technical background. This talk is your introduction to getting involved in the open source world and making it an even better place. Let magic happen to you.",Community (25 minutes),
PGCONF_7059,PostgreSQL Tips & Tricks For App Devs,Chris Ellis,"PostgreSQL has a huge range of features, maybe too many. Making use of these features can often make application developer's lives easier, reducing the complexity of their application. We'll take a look at some use cases I've ran into over the years and what features of PostgreSQL can be used to help solve those problems. Taking a look at use cases such as: * Event scheduling & booking * Task execution * Searching * Geolocation * Unknown data And more!",App Developer (45 minutes),
PGCONF_6979,"Managing Postgres at scale: Challenges, Tools & Techniques",Karen Jex,"Managing databases at scale can be hard! Databases keep getting bigger, and organisations are managing more and more databases. To make things even more challenging, you may be one of the many people who are expected to implement and look after database systems who isn’t actually a database administrator. Maybe your training and expertise lies in other areas such as sysadmin, devops or application development. In this session, we’ll think about some of the difficulties you might encounter when managing databases at scale, whether that means looking after multi-terabyte databases or being responsible for hundreds or even thousands of databases. More importantly, we’ll look at the skills, tools and techniques that you can rely on to manage them successfully. This will include topics such as: • architecture • automation • disaster recovery, high availability & backups • sizing, memory and storage • partitioning and sharding options • monitoring and alerting",DBA (45 minutes),
PGCONF_7037,"The SyncRep Detective Story: Chasing Ghosts in PostgreSQL, Finding Demons in Storage",Dmitry Fomin,"In a high-availability PostgreSQL architecture with sync replicas, a sudden spike in commit latency is a high-stakes crime. In our case, data from pg_wait_sampling pointed to a single, obvious suspect: the SyncRep wait event. This launched a critical investigation to restore our application's performance. Our initial inquiry took us down a rabbit hole of what we now know was a classic red herring. We were convinced the culprit was network-related, spending valuable time investigating potential network misconfigurations and signs of overload. Yet, despite our best efforts, the evidence never fully added up, and the performance issues persisted. The breakthrough came when we looked closer at the replica's activity and discovered our ""smoking gun"": persistently slow fdatasync() calls. This crucial clue exonerated the network and proved the problem wasn't in the transit of data, but in its persistence. The focus of our investigation pivoted instantly and intensely towards the storage layer, where the real mystery lay. This presentation details our two-pronged counterattack: The Tactical Containment: As we began the deep-dive into the storage system, we immediately implemented quorum-based synchronous commits. This provided crucial first-aid, reducing the immediate impact on the primary and buying us valuable time. The Strategic Solution: The core of our story is a deep dive into the custom patch we engineered for PostgreSQL. We will walk through the logic of this patch, designed specifically to reduce the primary's performance sensitivity to slow replica disk writes. Join this session to learn: How to use low-level diagnostics to spot slow fdatasync() calls and correctly identify I/O-bound replication lag. The practical benefits and implementation details of using quorum commits as a powerful mitigation strategy. A technical breakdown of our PostgreSQL patch and the lessons we learned in modifying the source to solve a unique infrastructure challenge. This is a real-world story of misdirection, deep-dive forensics, and pragmatic engineering. You will leave with a playbook for diagnosing the true source of replication latency, even when all initial signs point elsewhere.",Postgres Internals (45 minutes),
PGCONF_7191,TDE as an Extension: A Different Path for PostgreSQL Encryption,Zsolt Parragi,"Transparent Data Encryption (TDE) has been a long-standing challenge in the PostgreSQL community. While proprietary solutions exist and major patch sets have been proposed, the topic continues to spark debate on the hackers mailing list, with no clear path forward. Our team decided to take a different approach: instead of building TDE directly into PostgreSQL, we explored how far we could go by implementing it as an extension, pushing core changes only where extensibility improvements were needed. This has been, and still is, a demanding project. Along the way, we have built multiple prototypes, hit dead ends, and uncovered design trade-offs that were not obvious at the start. In this talk, we will share the technical lessons from our journey: what failed, what succeeded, how our extension-based approach actually works, and which challenges remain unsolved.",Sponsors,
PGCONF_7054,Implementing Slowly Changing Dimensions in Postgres,Marc Linster,"Slowly Changing Dimensions (SCD) are a popular method to model changes of database reference values over time and maintain history data, for example product prices that change over time. When correctly used, they are powerful and provide a structured way to simplify complex data models. This talk will review six types of SCDs and discuss how the most relevant type can be modeled in Postgres, and how JSONB, the daterange data type, and the btree_gist extension can be used to create very elegant and powerful SCD implementations in Postgres.",App Developer (25 minutes),
PGCONF_7095,Implementing strong cross-platform resiliency in agile environments with logical replication,Andreas Geppert,"Resiliency means that data of critical applications are not lost even in cases of extended and complete downtime of infrastructure providers. In particular, we have to guarantee that no more than 15 minutes worth of data are lost in such a scenario. The issue is exacerbated by the fact that applications may release schema changes at any point in time, without the possibility to allow deployments only during release windows. The obvious solution - physical replication - is not possible because it is not supported by the infrastructure provider. In this talk we show how logical replication in Postgres can be used to implement the required level of resiliency and how we deal with the fact that schema changes are not replicated.",DBA (25 minutes),
PGCONF_7021,They grow up so fast: donating your open source project to a foundation,"Floor Drees, Gabriele Bartolini","The first commit to the CloudNativePG project was made in February 2020. Just two years later, EDB began the process of donating the project to the Cloud Native Computing Foundation (CNCF), part of the nonprofit Linux Foundation. This move wasn’t just symbolic—it was a deliberate strategy to ensure CloudNativePG could continue to thrive under the guidance of a broader, more diverse community of contributors. Transferring stewardship of an open source project to a foundation is a milestone. It’s about more than governance—it’s about trust, neutrality, and long-term sustainability. Becoming part of the CNCF unlocks new levels of visibility, collaboration, and adoption, connecting the project with a global ecosystem of practitioners and end users. At the time of writing, CloudNativePG is a CNCF Sandbox project. Our hope is to reach Incubation status by the end of next year. But how did we get here—and what does “donating” a project really involve? Join us for a candid, behind-the-scenes look at the journey of donating an open-source project to a foundation: the motivations, the process, the surprises, and the lessons learned along the way.",Community (25 minutes),
PGCONF_6848,Long Queries and the Art of Full Scan,Henrietta Dombrovskaya,"Some queries just can’t run in a fraction of a second, no matter how well-written. This does not mean they cannot be optimized. Many practitioners hold that since analytical reports do not have strict response time requirements, it is not important how fast or slow they run. What's a big deal if a query only runs once a day, or once a week, or once a month? This is a dangerous practice. If report performance is neglected, performance can easily degrade from minutes to hours or even days! Let's learn how to be proactive and build reports the right way from the start, so that they won't become a performance disaster when the data volumes increase!",App Developer (45 minutes),
PGCONF_6931,Beyond the How: Why Table Partitioning Truly Matters,Derk van Veen,"Table partitioning has become a well-known technique in the PostgreSQL world. There have been talks about it at most of the conferences I visited last year. But often, we talk about the technicalities of partitioning and not why we partition. This talk is all about the ""why"" and will set you up for success when you are doing partitioning! Table partitioning starts with finding the leading figure in your data. Only when you find your leading figure can you partition all tables in conjunction with each other. And when all tables are partitioned around that leading figure, you have to align all the partition boundaries as well! Aligning partitions around your leading figure is crucial for basically all the good that partitioning can offer you. They open the door for: - At least not terrible performance - Timely partitioning maintenance - Out-of-the-box PII compliance - Trivial data cleanup - Transparent data tiering, moving data to a second storage tier without the application even being aware I am especially enthusiastic about how easy data tiering becomes after all the tough groundwork we covered over the years. When partitioning is done wrong, as I did it in the past, performance can and will be terrible. When done in the right way, you can use enable_partitionwise_join and enable_partitionwise_aggregate to make query performance with partitions really work. For me, the difference in performance was a factor of 70 with respect to query run times! All the applied strategies presented have been battle tested on our biggest, multiple +100TBs, and most pressured databases. As we are processing financial data they are compliant with the strict financial regulations and are fully based on out of the box and open source technologies. After this presentation, you will understand all the boundary conditions for building a high-performing, flexible and easy to manage partitioning plan. As a bonus, you get an easier environment to manage, faster vacuums, out-of-the-box PII data compliance, and a very natural way of data cleanup and/or tiering. You and your databases will be ready for whatever data volumes the future will throw at you!",DBA (45 minutes),
PGCONF_7001,AIO in PG 18 and beyond,Andres Freund,"PostgreSQL 18 gained support for using asynchronous I/O (AIO) for reads. This talk will discuss the limitations of AIO in PG 18, the workloads in which it can and can't help, and by how much it can help. After that we will look ahead to AIO support beyond PG 18.",Postgres Internals (45 minutes),
PGCONF_7185,From Chaos to Compliance: How PostgreSQL Makes Regulations Work for You,Raj Verma,"Compliance often feels like a burden, endless checklists, security audits, and regulations that threaten to slow innovation. But what if compliance could become a strength instead of a roadblock? In this talk, we’ll explore how PostgreSQL helps organizations meet GDPR, PCI DSS, HIPAA, and other regulatory requirements while keeping systems efficient and scalable. You’ll discover built-in features, practical extensions, and real-world strategies that turn compliance into an enabler of trust, security, and business growth. 1. How PostgreSQL features like RLS, auditing, and encryption make compliance easier than you think 2. Real-world patterns for tackling GDPR, PCI DSS, and HIPAA with PostgreSQL 3. Common compliance pitfalls (and how to avoid them before auditors find them) 4. Turning compliance into a business advantage instead of a bottleneck",Sponsors,
PGCONF_7094,"Table repacking, done right","Álvaro Herrera, Antonin Houska","There's always been a need for a mechanism to get rid of table bloat. VACUUM FULL does that already, but it comes with a potentially long period during which the table cannot be accessed or modified. Users discover to much frustration that Postgres does not offer any solutions to this problem, and they eventually turn to external tools. Enter REPACK CONCURRENTLY. Intended to be integrated in Postgres 19, we hope it will offer peace of mind to thousands of DBAs around the world. But what is it? How does it work? We intend to explain all that every user will need to know about it, and then some.",DBA (45 minutes),
PGCONF_7084,"Migrating to PostgreSQL: Strategies, Sovereignty, and Success",Raphael Salguero,"Migrating existing database systems can be one of the most complex yet rewarding endeavors for any organization. With PostgreSQL's rising popularity due to its robust feature set, open-source flexibility, and cost-effectiveness, many are looking to transition from proprietary databases like Oracle, MySQL, and Microsoft SQL Server. This 45-minute talk provides a comprehensive guide to navigating the intricate landscape of database migrations to PostgreSQL. We will delve into various migration strategies, from lift-and-shift to phased approaches, helping you choose the right path for your specific needs. A critical focus will be placed on data sovereignty aspects, discussing how to ensure compliance and maintain control over your data throughout the migration process. We'll explore a range of tooling options, from native PostgreSQL utilities to third-party solutions, and share best practices to minimize downtime and ensure data integrity. Attendees will gain insights into common pitfalls and learn how to avoid them, backed by real-world examples and live demonstrations. Whether you're planning your first migration or looking to optimize an ongoing transition, this session will equip you with the knowledge and confidence to achieve a successful and secure move to PostgreSQL.",DBA (45 minutes),
PGCONF_6887,"MultiXacts in PostgreSQL: usage, side effects, and monitoring",Divya Sharma,"PostgreSQL’s ability to handle concurrent access while maintaining data consistency relies heavily on its locking mechanisms, particularly at the row level. When multiple transactions attempt to lock the same row simultaneously, PostgreSQL turns to a specialized structure called MultiXact IDs. While MultiXacts provide an efficient way to manage multiple locks on a single row, they can also introduce performance challenges, such as unexpected slowdowns or delays during vacuum operations. In this talk, we dive deep into the inner workings of MultiXacts, exploring how they function, where they are commonly used, and the potential side effects they can have on your database performance. You’ll learn how to monitor MultiXacts, identify bottlenecks, and implement strategies to mitigate their impact on your system.",Postgres Internals (45 minutes),
PGCONF_7186,Pushing Postgres Forward: From Open Source Contributions to Intelligent Tuning,Adam Wolk,"Microsoft’s contributions to PostgreSQL continue to grow, from upstream performance improvements to innovations in Azure Database for PostgreSQL. This talk will focus on our open source work, with a deep dive into automated index tuning—how it works, what it enables, and where it’s going. Expect technical depth, real-world insights, and a look at how Microsoft is helping shape the future of PostgreSQL.",Sponsors,
PGCONF_7093,Diofanti.org: A Civic Tech Journey Powered by PostgreSQL,Florents Tselai,"What began as a simple script to track my mom’s citizenship case evolved into Diofanti.org—a free civic tech platform monitoring millions of transactions and decisions across the Greek government, all powered by PostgreSQL. In this talk, I’ll share how PostgreSQL—and its vibrant extension ecosystem—enabled every stage of this unlikely civic tech journey: - **Phase 1: The Scrappy Script** Structuring chaotic government PDFs into clean, queryable data using PostgreSQL. - **Phase 2: Scaling Transparency** Ingesting massive public datasets via the http extension and custom SQL pipelines. - **Phase 3: AI for the People** Powering a chatbot with pgvector-backed semantic search to help citizens ask real questions about their government—and their money. What started as a personal side project quickly pulled me deeper into PostgreSQL development. Diofanti’s challenges led me to write custom extensions like pgpdf, explore SQL/JSON path internals, and eventually contribute to PostgreSQL core itself. Whether you’re a developer, open-data advocate, or just someone who loves bending PostgreSQL to your will, this is a story of how a “small script” turned into a platform for civic impact.",App Developer (25 minutes),
PGCONF_6832,All about Common Vulnerabilities and exposures in PostgreSQL,Priyanka Chatterjee,"Security is always of utmost priority in the computing world and Postgres implements it in several ways. One such way is by having Roles and privileges. The highest privileged role is that of a Superuser. A role that is not a superuser is usually restricted from performing some actions, whether it is installing certain extension or viewing certain tables or altering the database state, unless granted to do so by a superuser itself. However, since Postgres is an open source community product that is constantly adding new features and to run a project using Postgres database involves not only the Postgres binary but hardware and OS on which it is hosted, the application framework , client libraries and various extensions installed, this can sometimes lead to some bugs that enable a non privileged user to perform certain actions or access certain data which it is not granted to. This risks a hacker mounting attacks with serious impact. Therefore it is very important for the DBAs to know how to navigate the CVE landscape, check for CVE announcements and decipher details to analyze impact on his environment and implement minor fixes whenever applicable. This is also important for users who are hosting their databases on Cloud and check with their cloud providers that the environment is secure and the such vulnerabilities are handled. My talk will be about: - What is CVE landscape: CVE, ADP, CVSS, CWE, CNA - Why it is important? - What Postgres projects are currently included? - Postgres Security Releases : timeline, where to check and what to check , enabling notification - How to report if a user finds a security bug and what actions will follow - Some example CVEs",DBA (25 minutes),
PGCONF_6932,Don't leak user data to AI - Strategies for protecting PII from LLMs and MCP,Jay MIller,"MCP has made it incredibly easy for large language models to connect to tools and services, including postgres. With many of the MCP servers that interact with postgres, you can speak to your databases using a combination of everyday language, prompting, and SQL. But over-eager-to-please agents could have your LLM reaching for parts of the Database that you didn't intend for it to look at. This can result in customer information being handed to LLMs which can not only result in unintentional data leaks, but also also violate the law. This talk reveals some strategies that protects you from LLMs getting hold of PII.",App Developer (25 minutes),
PGCONF_7182,"Postgres on AWS: Run 5X faster on metal, migrate anywhere",Ozgun Erdogan,"We'll explore how open-source managed PostgreSQL can deliver both superior performance and portability. By using local NVMe storage instead of network-attached disks, Ubicloud Postgres eliminates the I/O bottlenecks that limit performance in services like AWS RDS. This keynote covers how Ubicloud Postgres runs on EC2 instances with automated backups, recovery, high availability, and more. It also demonstrates how to achieve up to 5x higher performance.",Platinum Sponsor Keynotes,
PGCONF_7189,Why PostgreSQL took the crown from MySQL and what lies ahead,Peter Zaitsev,"In the early 2000s, MySQL was the go-to Open Source database for new projects. Now, two decades later, that's no longer the case. PostgreSQL has become the undisputed leader, winning the trust and mindshare of developers everywhere.So, how did this shift happen? This talk will explore what caused PostgreSQL's rise and MySQL's relative decline. We'll also look at the future of PostgreSQL and the challenges it needs to navigate to maintain its position as the top open-source database.",Platinum Sponsor Keynotes,
PGCONF_7183,PostgreSQL in ever more critical environments,Hans-Jürgen Schönig,PostgreSQL has grown and is now powering highly critical platforms. But what are the challenges and what is needed to achieve success? Let us dive in and find out ...,Platinum Sponsor Keynotes,
PGCONF_7102,Transform your development workflow with thin cloning,Nicholas Meyer,"Have you ever run into: - Schema differences between development and production? - Slow queries in production, but they were fast in development? - Bugs that happen in production but do not replicate in development, because only the exact way the data exists in production causes the bug? At Academia.edu, we have experienced all of the above. Wouldn't it be great if we could just debug everything in production? With database ""thin cloning"" you (kind of) can! In this talk, we will cover what ""thin cloning"" is, how you can use tools like DBLab Engine or Aurora cloning, and how it has transformed development workflows at Academia.edu.",App Developer (45 minutes),
PGCONF_7016,PostgreSQL as a Graph Database: Who Grabbed a Beer Together?,Taras Kloba,"Graph structures can reveal patterns in connected data that traditional relational models miss, such as how communities form and evolve over time. In this session, we will explore why graph modeling is important for these types of analyses and how PostgreSQL can serve as a powerful graph database. We will compare available extensions including Apache AGE, pgRouting, and pgGraph, and discuss their strengths and recommended use cases. Then, through a live demo using data about speakers from previous PGConf events combined with Untappd check-ins, we will visualize social interactions and shared experiences, showing how to uncover insights like who connected with whom, popular gathering spots, and how the conference community network changes over time, all with PostgreSQL.",Community (45 minutes),
PGCONF_6933,Hazards of logical decoding in PostgreSQL,Polina Bungina,"Logical decoding was introduced in PostgreSQL 9.4 and is being constantly improved. It has enabled many new applications. The extensible mechanism of output plugins and in-core logical replication based on logical decoding make this feature truly exciting. One popular use case of this feature is implementing the Change Data Capture (CDC) pattern. Although developers are usually eager to adopt this modern approach to leverage its apparent benefits, I strongly believe that using logical decoding in PostgreSQL requires at least a basic understanding of its implementation and should be treated with great care. In this talk, I will share the problems we, as Zalando's DBaaS team, encountered while adopting and maintaining our internal CDC solution based on logical decoding. I will also highlight the nuances you should always keep in mind when using this technology.",Postgres Internals (45 minutes),
PGCONF_7192,Lessons from two decades of hacking the proprietary value into open source databases,Michal Nosek,"Two decades of working with different open source databases have taught us a lot about what it takes to make them enterprise-ready while keeping them truly open. Features like high availability, Transparent Data Encryption (TDE) and OpenID Connect (OIDC) aren’t just “nice to have” — they’re what determine whether a database can succeed in the Enterprise environment. In this talk, we’ll explore how those lessons apply to PostgreSQL. We’ll discuss where Postgres follows familiar patterns from other open source databases, where it stands apart, and what that means for adoption in large organizations. We’ll also dive into the current state of making PostgreSQL fit for enterprise workloads, highlight ongoing gaps, and share ideas for what the community could focus on to ensure PostgreSQL’s continued growth and success.",Sponsors,
PGCONF_7045,An Introduction to B-trees and LSM-trees for DEVs and DevOps,Dave Pitts,"This introductory talk for DEVs and DevOps will focus on * Why you’re likely to be working with both B-trees and LSM-trees in the future * Key differences between B-trees and LSM-trees, and surprising gotchas when switching workloads between them",App Developer (25 minutes),
PGCONF_6891,Fast-path locking improvements in PG18,Tomas Vondra,"Let's talk about the improvement in fast-path locking in upcoming PG18, which can bring substantial speedups on systems with many cores and/or queries accessing many relations (where ""many"" may mean ""more than 16"", and it includes indexes). This is particularly visible with partitioning, but easy to hit even without it. I'll start by explaining how fast-path locking works, why it was introduced, and why it doesn't help with more relations. Then I'll explain how it was extended in PG18, and present some benchmark numbers to quantify the improvements.",Postgres Internals (25 minutes),
PGCONF_6990,Waiting for Commit,Ants Aasma,"Picking between synchronus or asynchronuos replication sounds like a simple question, but it has some deep implications for durability and performance. In this talk I will discuss why you should or should not pick synchronous replication. To see this we will go on a journey that a commit takes through a replicated cluster and all the hazards it meets on the way. How head-of-line blocking can escalate minor storage tail-latency issues on replica to a major performance problem for the whole system. What tools you can use to track down the source of the problem and what solutions are available to mitigate it.",DBA (25 minutes),
PGCONF_7018,Patroni: what the blog posts don't tell you...,Cameron Murdoch,"Patroni has become the Gold Standard for managing High Availability Postgres clusters. It has great documentation and there are hundreds of blog posts detailing how to set up your first cluster. However, there is a large gap between the blog posts and having a production ready system that you actually trust. This talk will briefly describe a typical Patroni setup before discussing/demonstrating the following topics that the blog posts never have enough space for! - Security hardening for Patroni, HAProxy, etcd and Postgres - Proxy layer considerations - Configuration best practice: what goes in the DCS? - Tips and tricks for managing several Patroni clusters - Backups - Upgrades - Failover testing Whether you are looking to build your first Patroni cluster or you already run several, there should be something in this talk for you!",DBA (45 minutes),
PGCONF_7128,"The Good, The Bad, and The Bloated: An Autovacuum Story",Jérémie Grauer,"Picture this: 3 autovacuum workers facing down 1,500 tables in a deadly standoff. The result? A bloated, slow database that forced a midnight dump/restore showdown. This talk will tell you everything you need to know about autovacuum: from the fundamentals of how it works to real-world production disasters to smart tuning strategies. You'll see what happens when autovacuum configurations go wrong, learn proper tuning approaches that keep databases running smoothly, and discover how PostgreSQL's observability helps you become the sheriff your databases need. Starting with autovacuum internals, we'll walk through actual production scenarios (including that 3-vs-1500 shootout), demonstrate how to spot trouble before it starts, and provide concrete strategies for turning autovacuum from potential foe into reliable friend. You'll leave knowing how to configure autovacuum properly for your workload, use PostgreSQL's observability tools effectively, and ride off into the sunset with a perfectly tuned database.",DBA (45 minutes),
PGCONF_7140,Panel Discussion: How to Work with Other Postgres People,"Boriss Mejias, Floor Drees, Jimmy Angelakos, Karen Jex","Lack of awareness around Mental Health issues has always been a problem in IT circles, leading to stress, burnout and difficulties in workplace and community relationships. This often results in employee turnover and strained dynamics within open source projects, and creates barriers for achieving a diverse contributor base. Building on the recognition that we are all different, this panel will address the challenges faced by contributors within the PostgreSQL ecosystem. Drawing on insights into neurodivergence and mental health contributed by community members, we'll explore tangible strategies for fostering a culture that promotes psychological well-being and respect for diverse needs and communication styles. Emphasizing practical tools like personal ""README"" guides, supportive communication practices, and conflict resolution techniques, the discussion aims to identify actionable ways to mitigate common stress points and support contributors effectively. We hope to make PostgreSQL a leading example of inclusivity and collaboration in open source, attracting and retaining a community of contributors who feel respected, supported, and empowered.",Community (45 minutes),
PGCONF_7190,"Monitoring Managed Postgres, the OSS Way",Shikhar Bhardwaj,"Observing and analyzing PostgreSQL behavior to identify performance bottlenecks is a key feature for a managed Postgres offering. In this talk, we will walk through a simple yet powerful setup to monitor PostgreSQL server performance in a managed cloud offering, based on open-source tools like postgres_exporter, VictoriaMetrics and Grafana. Using this setup, we will explore a methodology to decide which OS and Postgres metrics matter most, along with key signals to use for alerting. Lastly, we will go through some examples on how to investigate performance issues, with a focus on some new areas of adoption like vector storage.",Sponsors,
PGCONF_7120,Building Tetris in a SQL Query!,Nuno Faria,"Recursive CTE queries have been gaining special attention in recent years, as they enable complex computations to occur closer to the data source, rather than loading large amounts of data to an external application. This presentation aims to shed light on recursive CTEs in PostgreSQL, including use cases, key considerations when using them, and current limitations. To exemplify just how powerful CTEs are, a fully working game of Tetris is implemented in a PostgreSQL query.",App Developer (25 minutes),
PGCONF_7112,Tracking plan shapes over time with Plan IDs and the new pg_stat_plans,Lukas Fittl,"In this session I'll share an overview of my recent work to improve plan ID tracking in Postgres (similar to query ID tracking), and how it enables aggregate analysis of which query used which plan structure. We'll discuss which parts of a plan should be considered significant, and how to deal with special cases such as partitioning (which can lead to many unique plan IDs), and which parts of this could be done better in-core, vs an extension. We will explore our new pg_stat_plans extension (similar in spirit to the same named but no longer maintained original pg_stat_plans) that builds on improvements in Postgres 18 to allow low-overhead monitoring of query plans over time. We'll also compare how the new pg_stat_plans relates to previous approaches, including the old pg_stat_plans and pg_store_plans open-source extensions, as well as proprietary implementations by cloud providers that I'm familiar with as a user.",DBA (25 minutes),
PGCONF_6877,How to explain PostgreSQL to kids,Naisila Puka,"What if your kid asked you ""What’s this Postgres thingy you work on?"". Well, you could say that PostgreSQL is like a giant toy box with lots of different compartments: blocks in one spot, balls in another, and puzzles in their own place. But kids don't stop at one question, and they are unapologetically honest: ""That's it? What's so cool about it?"" Then you have to think about how to explain extensibility and advanced SQL compliance to them ... You could try the following: This giant toy box can add new kinds of toys and modify existing ones — you can make your own special toys and put them in the box. That’s extensibility! It knows lots of ways to find toys — even tricky ones that are hiding under other toys, and it can follow smart instructions like “find all the red balls that are bigger than this size and were put in the box last week”. That's advanced SQL compliance! Hopefully your kid will be quite interested in what you work on at this point, but you know what's next - another question ""Cool — can I play with friends? Will the giant toy box fix my broken toys?"" ... Join this talk for some fun, relatable analogies and storytelling techniques to break down the Postgres database’s sophisticated ideas into understandable, engaging narratives with children! Whether you’re a parent, teacher, or developer, discover how to share your passion for the PostgreSQL database and inspire the next generation with creativity.",Community (25 minutes),
PGCONF_6866,The Lifecycle of a SELECT: A Glimpse into the Depths of PostgreSQL Internals,Sergey Dudoladov,"PostgreSQL practitioners often advise developers with recommendations like ""Always use EXPLAIN ANALYZE with BUFFERS"" or ""Run ANALYZE first"". However, these suggestions are rarely accompanied by clear explanations of why they matter. Inspired by the motto ""Knowledge of certain principles easily compensates for the lack of knowledge of certain facts,"" this talk sheds light on key PostgreSQL architectural concepts and their connection to common design and performance best practices. Through a series of increasingly complex SELECT queries, we will explore how PostgreSQL’s internal mechanisms enable safe, fast, and efficient data processing. This session is designed for application developers who want to deepen their understanding of how PostgreSQL executes queries—and how to harness its full potential without accidentally bringing it to its knees.",App Developer (45 minutes),
PGCONF_7105,Breaking PostgreSQL - Learning by doing it wrong,Daniel Westermann,"Although PostgreSQL is a very reliable and robust database management system, there are things you shouldn't do. In this session we're breaking PostgreSQL in several ways, live, without any slides (slides will be provided anyway, don't worry). There is no better way to learn as learning from mistakes, isn't it? We're going to take that serious and I am sure you'll have a lot of fun and gain some interesting insights. By the end of this talk you should have a solid understanding of what not not do (and why not)and this will safe you quite some time in your journey with PostgreSQL.",DBA (45 minutes),
PGCONF_7010,We have multiple concurrent versions of this title trying to understand MVCC,Boriss Mejias,"When you create, update or delete a row in PostgreSQL, you are actually creating a new version of the row, hence, each row in PostgreSQL may have multiple versions during its life, sometimes many versions concurrently. These multiple versions are the key to different transaction isolation levels, consistent snapshots, and everything around concurrency control. Understanding how multi-version concurrency control (MVCC) works will open you many doors and windows about PostgreSQL internals, vacuum, and visibility of rows within a transaction. In this talk we will show with concrete examples the MVCC principle, how to observe it and how to monitor the impact it has with the entire system, as it is interconnected with locks, vacuum, and consistency. You will learn when to use transactions with “repeatable reads” or with “read committed” mode, to design better applications. We will see how MVCC is at the core of understanding Postgres.",Postgres Internals (45 minutes),
PGCONF_7188,"Delivering High Availability, Reliability, Scalability, and Low Latency for PostgreSQL Deployments of Any Size",Ahsan Hadi,"pgEdge believes in the power of PostgreSQL and everything that has contributed to it. That's why we've provided pgEdge Distributed PostgreSQL entirely under the standard PostgreSQL license and releasing pgEdge Enterprise Postgres, to bring you 100% open source, standard PostgreSQL fully optimized for high availability (for both distributed and non-distributed workloads), and more. Scale from a single instance of Postgres, easily implement high availability with read replicas and Patroni, and ultimately grow to globally distributed PostgreSQL architecture across multiple geographies for extreme high availability and low latency requirements. Move with freedom knowing your infrastructure will grow easily with your organization. We'll be here to support you along the way. Attend this session to learn how to leverage pgEdge Enterprise PostgreSQL to support your operations and scale from any point, whether you're running a single-instance or already hosting a fleet of deployments.",Sponsors,
PGCONF_7082,Can PostgreSQL Compete with Analytics Specialists?,Lætitia AVROT,"Can PostgreSQL really compete with DuckDB and ClickHouse for analytics workloads? The answer might surprise you! This session examines PostgreSQL's capabilities in the analytics space and how it stacks up against specialized engines. We'll explore what's possible when you properly tune PostgreSQL for analytical workloads and discover where it excels versus where purpose-built tools shine. Beyond raw performance comparisons, we'll tackle the critical decision every team faces: when should you stick with PostgreSQL and when is it time to adopt specialized tools? Through practical decision criteria and real-world considerations, we'll establish clear guidelines that go beyond benchmarks. PostgreSQL's versatility, operational simplicity, and massive ecosystem often make it the pragmatic choice for far more analytics scenarios than commonly believed. Stop assuming you need specialized tools for every analytical workload. Learn when PostgreSQL is ""good enough"" and when it's time to reach for purpose-built analytics engines. Sometimes the most pragmatic choice is sticking with the reliable workhorse you already know and trust.",App Developer (45 minutes),
PGCONF_7116,CTRL+Z for your Database : Mastering Backup and Recovery in PostgreSQL,Sakshi Nasha,"What if you could hit CTRL+Z when your production database crashes, someone drops a critical table, or a cloud region outage takes your system offline? In this talk, we’ll explore how to build a disaster-resilient PostgreSQL system, one that not only backs up data efficiently but also recovers gracefully when the unexpected happens. From traditional SQL dumps to the new incremental backup feature introduced in PostgreSQL 17, we’ll cover practical strategies you can use in real-world environments. Key Takeaways : 1) Why backups matter and how to choose the right type for your use case 2) PostgreSQL backup options: logical vs physical, full vs incremental 3) Point-in-Time Recovery (PITR), WAL archiving, delayed replicas, and failover testing 4) What's new in PostgreSQL 17: pg_basebackup with incremental mode, pg_combinebackup, and --filter in pg_dump 5) How to design PostgreSQL systems for durability, high availability, and quick recovery Whether you're managing a single instance or a fleet of databases in production, this session will help you build a resilient PostgreSQL environment ready to handle real-world challenges with confidence and control.",DBA (45 minutes),
PGCONF_6825,Database in Distress: Testing and Repairing Different Types of Database Corruption,Josef Machytka,"In recent years, I had to repair several corrupted PostgreSQL databases and have seen many different problems. In some cases corrupted data can even crash the session reading it. To better understand these issues and test different strategies for repairs, I created a Python application that surgically creates various types of damages. This talk demonstrates, through practical examples and outputs from the pageinspect extension, different types of data corruption. Why we see specific errors and how to diagnose these issues. PostgreSQL 18 sets checksums ON by default in initdb, therefore talk also shows how checksums can help to detect damaged pages and ""zero them out"". It also proposes some improvements that could help to handle corrupted data more effectively on already existing databases which do not use checksums. Takeaways: - PostgreSQL databases can suffer from various types of corruption - Understanding heap table structure is essential for analyzing different types of errors - Data page checksums are the best tool for skipping damaged pages - Without checksums only pages with clearly corrupted header can be removed using the zero_damaged_pages = on setting - Other types of corruption might require time-consuming salvaging operations - Improvements in PostgreSQL code and/or new settings could make some repairs less painful on databases without checksums",Postgres Internals (45 minutes),